{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers, layers\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
    "    Output shape: [None, d2]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
    "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of vectors of capsules\n",
    "            x = inputs\n",
    "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
    "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
    "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
    "\n",
    "        # masked inputs, shape = [batch_size, dim_vector]\n",
    "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
    "        return inputs_masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][-1]])\n",
    "        else:\n",
    "            return tuple([None, input_shape[-1]])\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\"  \n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        \n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        \"\"\"\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        \"\"\"\n",
    "        # Routing algorithm V1. Use tf.while_loop in a dynamic way.\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "            b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            return [i-1, b, outputs]\n",
    "\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars)\n",
    "        \"\"\"\n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
    "    return layers.Lambda(squash)(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 4d, [None, width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param num_routing: number of routing iterations\n",
    "    :return: A Keras Model with 2 inputs and 2 outputs\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
    "    x_recon = layers.Dense(512, activation='relu')(masked)\n",
    "    x_recon = layers.Dense(1024, activation='relu')(x_recon)\n",
    "    x_recon = layers.Dense(784, activation='sigmoid')(x_recon)\n",
    "    x_recon = layers.Reshape(target_shape=[28, 28, 1], name='out_recon')(x_recon)\n",
    "\n",
    "    # two-input-two-output keras Model\n",
    "    return models.Model([x, y], [out_caps, x_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 17:31:48.210857  3324 deprecation_wrapper.py:119] From C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0807 17:31:48.228443  3324 deprecation_wrapper.py:119] From C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0807 17:31:48.231960  3324 deprecation_wrapper.py:119] From C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0807 17:31:48.401806  3324 deprecation.py:506] From <ipython-input-2-8daadbc5443a>:132: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "W0807 17:31:48.444518  3324 variables.py:2429] Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1152, 8)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1152, 8)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 10, 16)       1486080     lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 16)           0           digitcaps[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          8704        mask_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         525312      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 784)          803600      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_caps (Length)               (None, 10)           0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Reshape)             (None, 28, 28, 1)    0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,153,360\n",
      "Trainable params: 8,141,840\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n",
      "No fancy plot Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = CapsNet(input_shape=[28, 28, 1],\n",
    "                n_class=10,\n",
    "                num_routing=3)\n",
    "model.summary()\n",
    "try:\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "except Exception as e:\n",
    "    print('No fancy plot {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train = pd.read_csv('C:/pythonwork/fashionmnist/fashion-mnist_train.csv')\n",
    "X_full = data_train.iloc[:,1:]\n",
    "y_full = data_train.iloc[:,:1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_full, y_full, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (42000, 28, 28, 1) 1.0\n",
      "Testing (18000, 28, 28, 1) 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.values.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "x_test = x_test.values.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "y_train = to_categorical(y_train.astype('float32'))\n",
    "y_test = to_categorical(y_test.astype('float32'))\n",
    "print('Training', x_train.shape, x_train.max())\n",
    "print('Testing', x_test.shape, x_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epoch_size_frac=1.0):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    # unpacking the data\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger('log.csv')\n",
    "    checkpoint = callbacks.ModelCheckpoint('weights-{epoch:02d}.h5',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.0005],\n",
    "                  metrics={'out_caps': 'accuracy'})\n",
    "\n",
    "    \"\"\"\n",
    "    # Training without data augmentation:\n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint])\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------Begin: Training with data augmentation -----------------------------------#\n",
    "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "                                           height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "    train_history=model.fit_generator(generator=train_generator(x_train, y_train, 64, 0.1),\n",
    "                        steps_per_epoch=int(epoch_size_frac*y_train.shape[0] / 64),\n",
    "                        epochs=4,\n",
    "                        validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "                        callbacks=[log, checkpoint, lr_decay])\n",
    "    # -----------------------------------End: Training with data augmentation -----------------------------------#\n",
    "\n",
    "    model.save_weights('trained_model.h5')\n",
    "    print('Trained model saved to \\'trained_model.h5\\'')\n",
    "\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 17:31:54.421522  3324 deprecation_wrapper.py:119] From C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0807 17:31:54.799905  3324 deprecation.py:323] From C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0807 17:31:55.471248  3324 deprecation_wrapper.py:119] From C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0807 17:31:55.655164  3324 deprecation_wrapper.py:119] From C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "131/131 [==============================] - 44s 338ms/step - loss: 0.3347 - out_caps_loss: 0.3346 - out_recon_loss: 0.1596 - out_caps_acc: 0.5394 - val_loss: 0.2019 - val_out_caps_loss: 0.2018 - val_out_recon_loss: 0.1655 - val_out_caps_acc: 0.7000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20190, saving model to weights-01.h5\n",
      "Epoch 2/4\n",
      "131/131 [==============================] - 42s 318ms/step - loss: 0.1959 - out_caps_loss: 0.1958 - out_recon_loss: 0.1421 - out_caps_acc: 0.7277 - val_loss: 0.1672 - val_out_caps_loss: 0.1671 - val_out_recon_loss: 0.1183 - val_out_caps_acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20190 to 0.16720, saving model to weights-02.h5\n",
      "Epoch 3/4\n",
      "131/131 [==============================] - 42s 317ms/step - loss: 0.1744 - out_caps_loss: 0.1744 - out_recon_loss: 0.0878 - out_caps_acc: 0.7530 - val_loss: 0.1827 - val_out_caps_loss: 0.1827 - val_out_recon_loss: 0.0848 - val_out_caps_acc: 0.7167\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16720\n",
      "Epoch 4/4\n",
      "131/131 [==============================] - 42s 317ms/step - loss: 0.1592 - out_caps_loss: 0.1592 - out_recon_loss: 0.0811 - out_caps_acc: 0.7752 - val_loss: 0.1537 - val_out_caps_loss: 0.1537 - val_out_recon_loss: 0.0837 - val_out_caps_acc: 0.7667\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16720 to 0.15371, saving model to weights-04.h5\n",
      "Trained model saved to 'trained_model.h5'\n"
     ]
    }
   ],
   "source": [
    "train_history=train(model=model, data=((x_train, y_train), (x_test[:60], y_test[:60])), \n",
    "      epoch_size_frac = 0.2) # do 10% of an epoch (takes too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(np.sqrt(num))\n",
    "    height = int(np.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return image\n",
    "\n",
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n",
    "    print('-'*50)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "\n",
    "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
    "    image = img * 255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"real_and_recon.png\")\n",
    "    print()\n",
    "    print('Reconstructed images are saved to ./real_and_recon.png')\n",
    "    print('-'*50)\n",
    "    plt.imshow(plt.imread(\"real_and_recon.png\", ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Test acc: 0.8136666666666666\n",
      "\n",
      "Reconstructed images are saved to ./real_and_recon.png\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('C:/pythonwork/fashionmnist/fashion-mnist_train.csv')\n",
    "data_test = test_df.iloc[:,1:].values.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "data_test_y = to_categorical(test_df.iloc[:,:1])\n",
    "test(model=model, data=(data_test[:3000], data_test_y[:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_train_history(train_history, train, validation,):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_train_history(train_history, 'out_caps_acc', 'val_out_caps_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
