{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import activations\n",
    "from keras import utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vld_Splt= 0.0909\n",
    "batch_size = 200\n",
    "num_classes = 10\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\\\n",
    "         .astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1).\\\n",
    "         astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) \n",
      " (10000, 10) \n",
      " (60000, 28, 28, 1) \n",
      " (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, '\\n',y_test.shape,'\\n',\n",
    "      x_train.shape,'\\n', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the squashing function.\n",
    "# we use 0.5 in stead of 1 in hinton's paper.\n",
    "# if 1, the norm of vector will be zoomed out.\n",
    "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
    "# and be zoomed out while original norm is greater than 0.5.\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "# define our own softmax function instead of K.softmax\n",
    "# because K.softmax can not specify axis.\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n",
    "\n",
    "\n",
    "class Capsule(Layer):\n",
    "    \"\"\"A Capsule Implement with Pure Keras\n",
    "    There are two vesions of Capsule.\n",
    "    One is like dense layer (for the fixed-shape input),\n",
    "    and the other is like timedistributed dense (for various length input).\n",
    "\n",
    "    The input shape of Capsule must be (batch_size,\n",
    "                                        input_num_capsule,\n",
    "                                        input_dim_capsule\n",
    "                                       )\n",
    "    and the output shape is (batch_size,\n",
    "                             num_capsule,\n",
    "                             dim_capsule\n",
    "                            )\n",
    "\n",
    "    Capsule Implement is from https://github.com/bojone/Capsule/\n",
    "    Capsule Paper: https://arxiv.org/abs/1710.09829\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_capsule,\n",
    "                 dim_capsule,\n",
    "                 routings=3,\n",
    "                 share_weights=True,\n",
    "                 activation='squash',\n",
    "                 **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'squash':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(1, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(input_num_capsule, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
    "        but replace b = b + <u,v> with b = <u,v>.\n",
    "\n",
    "        This change can improve the feature representation of Capsule.\n",
    "\n",
    "        However, you can replace\n",
    "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
    "        with\n",
    "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
    "        to realize a standard routing.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.share_weights:\n",
    "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
    "        else:\n",
    "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(inputs)[0]\n",
    "        input_num_capsule = K.shape(inputs)[1]\n",
    "        hat_inputs = K.reshape(hat_inputs,\n",
    "                               (batch_size, input_num_capsule,\n",
    "                                self.num_capsule, self.dim_capsule))\n",
    "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
    "\n",
    "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
    "        for i in range(self.routings):\n",
    "            c = softmax(b, 1)\n",
    "            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(o, hat_inputs, [2, 3])\n",
    "                if K.backend() == 'theano':\n",
    "                    o = K.sum(o, axis=1)\n",
    "\n",
    "        return o\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 300)       3000      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 26, 26, 300)       0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 676, 300)          0         \n",
      "_________________________________________________________________\n",
      "capsule_2 (Capsule)          (None, 10, 16)            48000     \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 51,000\n",
      "Trainable params: 51,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"one-CONV-LAYERS-THICK\"\"\"\n",
    "input_image = Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "x_0 = Conv2D(filters= 300, kernel_size= 3, activation='relu', \n",
    "             padding= 'valid')(input_image)\n",
    "x_0= Dropout(0.1)(x_0)\n",
    "\n",
    "\n",
    "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "then connect a Capsule layer.\n",
    "\n",
    "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "\n",
    "the length of Capsule is the proba,\n",
    "so the problem becomes a 10 two-classification problem.\n",
    "\"\"\"\n",
    "\n",
    "x_0 = Reshape((-1, 300))(x_0)\n",
    "capsule_0 = Capsule(num_classes, 16, 3, True)(x_0)\n",
    "output_0 = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule_0)\n",
    "model_0 = Model(inputs= input_image, outputs= output_0)\n",
    "\n",
    "# we use a margin loss\n",
    "model_0.compile(loss =margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "model_0.summary()\n",
    "\n",
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 26, 26, 40)        400       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 26, 26, 40)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 24, 24, 100)       36100     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 24, 24, 100)       0         \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 576, 100)          0         \n",
      "_________________________________________________________________\n",
      "capsule_9 (Capsule)          (None, 10, 16)            16000     \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 52,500\n",
      "Trainable params: 52,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"two-CONV-LAYERS-THICK\"\"\"\n",
    "input_image = Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "x_1 = Conv2D(filters= 40, kernel_size= 3, activation='relu', \n",
    "             padding= 'valid')(input_image)\n",
    "x_1= Dropout(0.1)(x_1)\n",
    "x_1 = Conv2D(filters= 100, kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_1)\n",
    "x_1 = Dropout(0.1)(x_1)\n",
    "\n",
    "\n",
    "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "then connect a Capsule layer.\n",
    "\n",
    "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "\n",
    "the length of Capsule is the proba,\n",
    "so the problem becomes a 10 two-classification problem.\n",
    "\"\"\"\n",
    "\n",
    "x_1 = Reshape((-1, 100))(x_1)\n",
    "capsule_1 = Capsule(num_classes, 16, 3, True)(x_1)\n",
    "output_1 = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule_1)\n",
    "model_1= Model(inputs= input_image, outputs= output_1)\n",
    "\n",
    "# we use a margin loss\n",
    "model_1.compile(loss =margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "model_1.summary()\n",
    "\n",
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 26, 26, 25)        250       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 26, 26, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 24, 24, 50)        11300     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 24, 24, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 22, 22, 70)        31570     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 22, 22, 70)        0         \n",
      "_________________________________________________________________\n",
      "reshape_14 (Reshape)         (None, 484, 70)           0         \n",
      "_________________________________________________________________\n",
      "capsule_13 (Capsule)         (None, 10, 16)            11200     \n",
      "_________________________________________________________________\n",
      "lambda_13 (Lambda)           (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 54,320\n",
      "Trainable params: 54,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"three-CONV-LAYERS-THICK\"\"\"\n",
    "input_image = Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "x_2 = Conv2D(filters= 25, kernel_size= 3, activation='relu', \n",
    "             padding= 'valid')(input_image)\n",
    "x_2= Dropout(0.1)(x_2)\n",
    "x_2 = Conv2D(filters= 50, kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_2)\n",
    "x_2 = Dropout(0.1)(x_2)\n",
    "x_2 = Conv2D(filters= 70, kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_2)\n",
    "x_2 = Dropout(0.1)(x_2)\n",
    "\n",
    "\n",
    "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "then connect a Capsule layer.\n",
    "\n",
    "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "\n",
    "the length of Capsule is the proba,\n",
    "so the problem becomes a 10 two-classification problem.\n",
    "\"\"\"\n",
    "\n",
    "x_2 = Reshape((-1, 70))(x_2)\n",
    "capsule_2 = Capsule(num_classes, 16, 3, True)(x_2)\n",
    "output_2 = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule_2)\n",
    "model_2= Model(inputs= input_image, outputs= output_2)\n",
    "\n",
    "# we use a margin loss\n",
    "model_2.compile(loss =margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "model_2.summary()\n",
    "\n",
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 26, 26, 25)        250       \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 26, 26, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 24, 24, 35)        7910      \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 24, 24, 35)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 22, 22, 45)        14220     \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 22, 22, 45)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 20, 20, 50)        20300     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 20, 20, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_20 (Reshape)         (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "capsule_19 (Capsule)         (None, 10, 16)            8000      \n",
      "_________________________________________________________________\n",
      "lambda_19 (Lambda)           (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 50,680\n",
      "Trainable params: 50,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"four-CONV-LAYERS-THICK\"\"\"\n",
    "input_image = Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "x_3 = Conv2D(filters= 25, kernel_size= 3, activation='relu', \n",
    "             padding= 'valid')(input_image)\n",
    "x_3= Dropout(0.1)(x_3)\n",
    "x_3 = Conv2D(filters= 35 ,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_3)\n",
    "x_3 = Dropout(0.1)(x_3)\n",
    "x_3 = Conv2D(filters= 45, kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_3)\n",
    "x_3= Dropout(0.1)(x_3)\n",
    "x_3 = Conv2D(filters= 50,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_3)\n",
    "x_3= Dropout(0.1)(x_3)\n",
    "\n",
    "\n",
    "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "then connect a Capsule layer.\n",
    "\n",
    "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "\n",
    "the length of Capsule is the proba,\n",
    "so the problem becomes a 10 two-classification problem.\n",
    "\"\"\"\n",
    "\n",
    "x_3 = Reshape((-1, 50))(x_3)\n",
    "capsule_3 = Capsule(num_classes, 16, 3, True)(x_3)\n",
    "output_3 = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule_3)\n",
    "model_3= Model(inputs= input_image, outputs= output_3)\n",
    "\n",
    "# we use a margin loss\n",
    "model_3.compile(loss =margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "model_3.summary()\n",
    "\n",
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 26, 26, 25)        250       \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 26, 26, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 24, 24, 30)        6780      \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 24, 24, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 22, 22, 30)        8130      \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 22, 22, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 20, 20, 40)        10840     \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 20, 20, 40)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 18, 18, 50)        18050     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 18, 18, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_22 (Reshape)         (None, 324, 50)           0         \n",
      "_________________________________________________________________\n",
      "capsule_21 (Capsule)         (None, 10, 16)            8000      \n",
      "_________________________________________________________________\n",
      "lambda_21 (Lambda)           (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 52,050\n",
      "Trainable params: 52,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"five-CONV-LAYERS-THICK\"\"\"\n",
    "input_image = Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "x_4 = Conv2D(filters= 25, kernel_size= 3, activation='relu', \n",
    "             padding= 'valid')(input_image)\n",
    "x_4= Dropout(0.1)(x_4)\n",
    "x_4 = Conv2D(filters= 30 ,kernel_size= 3, activation='relu',\n",
    "             strides= 2)(x_4)\n",
    "x_4 = Dropout(0.1)(x_4)\n",
    "x_4 = Conv2D(filters= 30, kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_4)\n",
    "x_4= Dropout(0.1)(x_4)\n",
    "x_4 = Conv2D(filters= 40,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_4)\n",
    "x_4= Dropout(0.1)(x_4)\n",
    "x_4 = Conv2D(filters= 50,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_4)\n",
    "x_4= Dropout(0.1)(x_4)\n",
    "\n",
    "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "then connect a Capsule layer.\n",
    "\n",
    "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "\n",
    "the length of Capsule is the proba,\n",
    "so the problem becomes a 10 two-classification problem.\n",
    "\"\"\"\n",
    "\n",
    "x_4 = Reshape((-1, 50))(x_4)\n",
    "capsule_4 = Capsule(num_classes, 16, 3, True)(x_4)\n",
    "output_4 = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule_4)\n",
    "model_4= Model(inputs= input_image, outputs= output_4)\n",
    "\n",
    "# we use a margin loss\n",
    "model_4.compile(loss =margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "model_4.summary()\n",
    "\n",
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 26, 26, 25)        250       \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 26, 26, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 12, 12, 25)        5650      \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 12, 12, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 10, 10, 30)        6780      \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 10, 10, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 8, 8, 30)          8130      \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 8, 8, 30)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 6, 6, 40)          10840     \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 6, 6, 40)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 4, 4, 45)          16245     \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 4, 4, 45)          0         \n",
      "_________________________________________________________________\n",
      "reshape_25 (Reshape)         (None, 16, 45)            0         \n",
      "_________________________________________________________________\n",
      "capsule_24 (Capsule)         (None, 10, 16)            7200      \n",
      "_________________________________________________________________\n",
      "lambda_24 (Lambda)           (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 55,095\n",
      "Trainable params: 55,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"six-CONV-LAYERS-THICK\"\"\"\n",
    "input_image = Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "x_5 = Conv2D(filters= 25, kernel_size= 3, activation='relu', \n",
    "             padding= 'valid')(input_image)\n",
    "x_5= Dropout(0.1)(x_5)\n",
    "x_5 = Conv2D(filters= 25 ,kernel_size= 3, activation='relu',\n",
    "             strides= 2)(x_5)\n",
    "x_5 = Dropout(0.1)(x_5)\n",
    "x_5 = Conv2D(filters= 30, kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_5)\n",
    "x_5= Dropout(0.1)(x_5)\n",
    "x_5 = Conv2D(filters= 30,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_5)\n",
    "x_5= Dropout(0.1)(x_5)\n",
    "x_5 = Conv2D(filters= 40,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_5)\n",
    "x_5= Dropout(0.1)(x_5)\n",
    "x_5 = Conv2D(filters= 45,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_5)\n",
    "x_5= Dropout(0.1)(x_5)\n",
    "\n",
    "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "then connect a Capsule layer.\n",
    "\n",
    "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "\n",
    "the length of Capsule is the proba,\n",
    "so the problem becomes a 10 two-classification problem.\n",
    "\"\"\"\n",
    "\n",
    "x_5 = Reshape((-1, 45))(x_5)\n",
    "capsule_5 = Capsule(num_classes, 16, 3, True)(x_5)\n",
    "output_5 = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule_5)\n",
    "model_5= Model(inputs= input_image, outputs= output_5)\n",
    "\n",
    "# we use a margin loss\n",
    "model_5.compile(loss =margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "model_5.summary()\n",
    "\n",
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 26, 26, 25)        250       \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 26, 26, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 12, 12, 25)        5650      \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 12, 12, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 10, 10, 25)        5650      \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 10, 10, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 8, 8, 30)          6780      \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 8, 8, 30)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 6, 6, 30)          8130      \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 6, 6, 30)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 4, 4, 40)          10840     \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 4, 4, 40)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 2, 2, 40)          14440     \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 2, 2, 40)          0         \n",
      "_________________________________________________________________\n",
      "reshape_31 (Reshape)         (None, 4, 40)             0         \n",
      "_________________________________________________________________\n",
      "capsule_30 (Capsule)         (None, 10, 16)            6400      \n",
      "_________________________________________________________________\n",
      "lambda_30 (Lambda)           (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 58,140\n",
      "Trainable params: 58,140\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"seven-CONV-LAYERS-THICK\"\"\"\n",
    "input_image = Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "x_6 = Conv2D(filters= 25, kernel_size= 3, activation='relu', \n",
    "             padding= 'valid')(input_image)\n",
    "x_6= Dropout(0.1)(x_6)\n",
    "x_6 = Conv2D(filters= 25 ,kernel_size= 3, activation='relu',\n",
    "             strides= 2)(x_6)\n",
    "x_6 = Dropout(0.1)(x_6)\n",
    "x_6 = Conv2D(filters= 25, kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_6)\n",
    "x_6= Dropout(0.1)(x_6)\n",
    "x_6 = Conv2D(filters= 30,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_6)\n",
    "x_6= Dropout(0.1)(x_6)\n",
    "x_6 = Conv2D(filters= 30,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_6)\n",
    "x_6= Dropout(0.1)(x_6)\n",
    "x_6 = Conv2D(filters= 40,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_6)\n",
    "x_6= Dropout(0.1)(x_6)\n",
    "x_6 = Conv2D(filters= 40,kernel_size= 3, activation='relu',\n",
    "             strides= 1)(x_6)\n",
    "x_6= Dropout(0.1)(x_6)\n",
    "\n",
    "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "then connect a Capsule layer.\n",
    "\n",
    "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "\n",
    "the length of Capsule is the proba,\n",
    "so the problem becomes a 10 two-classification problem.\n",
    "\"\"\"\n",
    "\n",
    "x_6 = Reshape((-1, 40))(x_6)\n",
    "capsule_6 = Capsule(num_classes, 16, 3, True)(x_6)\n",
    "output_6 = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule_6)\n",
    "model_6= Model(inputs= input_image, outputs= output_6)\n",
    "\n",
    "# we use a margin loss\n",
    "model_6.compile(loss =margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "model_6.summary()\n",
    "\n",
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"one-CONV-LAYERS-THICK\"\"\"\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    train_history_0= model_0.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split= Vld_Splt,\n",
    "        shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by dataset std\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally\n",
    "        height_shift_range=0.1,  # randomly shift images vertically\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    train_history_0= model_0.fit_generator(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs= epochs,\n",
    "        validation_split= Vld_Splt,\n",
    "        steps_per_epoch= int(x_train.shape[0]/ batch_size),\n",
    "        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"two-CONV-LAYERS-THICK\"\"\"\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    train_history_1= model_1.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split= Vld_Splt,\n",
    "        shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by dataset std\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally\n",
    "        height_shift_range=0.1,  # randomly shift images vertically\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    train_history_1= model_1.fit_generator(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs= epochs,\n",
    "        validation_split= Vld_Splt,\n",
    "        steps_per_epoch= int(x_train.shape[0]/ batch_size),\n",
    "        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"one-CONV-LAYERS-THICK\"\"\"\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    train_history_2= model_2.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split= Vld_Splt,\n",
    "        shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by dataset std\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally\n",
    "        height_shift_range=0.1,  # randomly shift images vertically\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    train_history_2= model_2.fit_generator(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs= epochs,\n",
    "        validation_split= Vld_Splt,\n",
    "        steps_per_epoch= int(x_train.shape[0]/ batch_size),\n",
    "        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOUR-CONV-LAYERS-THICK\"\"\"\n",
    "scores_0= model_0.evaluate(x_test, y_test)\n",
    "scores_0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_history(train_history_0, 'acc', 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history_0.history['val_acc'], 'd-')\n",
    "plt.title('Training on MNIST')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['CapsNet: 5-conv'], loc='upper left')\n",
    "plt.ylim(.925, .997)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
